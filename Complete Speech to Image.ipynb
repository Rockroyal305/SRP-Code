{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "724ba302",
   "metadata": {},
   "source": [
    "### Read in audio file and ensure that the path is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750eba52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Find out which language to use\n",
    "lang_code = input(\n",
    "    \"What Language Do You Want to Use? (e.g. en-US, de-DE, fr-FR, hi-IN, zh-CN, etc;)\")\n",
    "if(lang_code != \"en-US\"):\n",
    "    print(\"Sorry, only english is supported!\")\n",
    "    quit()\n",
    "\n",
    "found_file = False\n",
    "file_path = input(\n",
    "    \"Enter the path to an audio file. \\n Please note that it must be a .wav file:\")\n",
    "while(found_file == False):\n",
    "    found_file = os.path.exists(file_path) and file_path[-3:] == \"wav\"\n",
    "    if(found_file == False):\n",
    "        file_path = input(\"That file-path was invalid, Please try again:\")\n",
    "print(\"File was succesfully input!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84a6eee",
   "metadata": {},
   "source": [
    "### Figure out the sampling rate of the .wav file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7957343",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.wavfile import read as read_wav\n",
    "\n",
    "sampling_rate, data = read_wav(file_path)\n",
    "print(\"The sampling rate of the file is: {} Hz.\".format(sampling_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8b83ea",
   "metadata": {},
   "source": [
    "### Upload the file to Google Cloud Storage for transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc411b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r\"C:\\Users\\rockr\\Desktop\\Code\\SRP\\sample-key.json\"\n",
    "\n",
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "    # Uploads a file to the bucket.\n",
    "\n",
    "    # The ID of your GCS bucket\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "\n",
    "    # The path to your file to upload\n",
    "    # source_file_name = \"local/path/to/file\"\n",
    "    \n",
    "    # The ID of your GCS object\n",
    "    # destination_blob_name = \"storage-object-name\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    # print(\n",
    "    #     \"File {} uploaded to {}.\".format(\n",
    "    #         source_file_name, destination_blob_name\n",
    "    #     )\n",
    "    # )\n",
    "    print(\"File was successfully uploaded to Google Cloud Storage!\")\n",
    "\n",
    "\n",
    "upload_blob(\"srp_speech_to_text\", file_path, \"audio_file_for_transcription.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a084c6c7",
   "metadata": {},
   "source": [
    "### Transcribe the file and create a list of all the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a512fba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import speech\n",
    "\n",
    "# Instantiates a client\n",
    "client = speech.SpeechClient()\n",
    "\n",
    "# The name of the audio file to transcribe\n",
    "gcs_uri = \"gs://srp_speech_to_text/audio_file_for_transcription.wav\"\n",
    "\n",
    "audio = speech.RecognitionAudio(uri=gcs_uri)\n",
    "\n",
    "config = speech.RecognitionConfig(\n",
    "    encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "    sample_rate_hertz=sampling_rate,\n",
    "    language_code=\"en-US\",\n",
    ")\n",
    "\n",
    "# Detects speech in the audio file\n",
    "response = client.recognize(config=config, audio=audio)\n",
    "\n",
    "sentences = []\n",
    "for result in response.results:\n",
    "    s = (format(result.alternatives[0].transcript))\n",
    "    if(s[0] == ' '):\n",
    "        s = s[:0] + s[1:]\n",
    "    # print(s)\n",
    "    sentences.append(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e4f2bd",
   "metadata": {},
   "source": [
    "### Turn each of the sentences into an image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208698dd",
   "metadata": {
    "id": "rf5pf_mwOqCm"
   },
   "source": [
    "### Install package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd19e6eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aL8du-leNk7F",
    "outputId": "1f9ea07a-8e00-442d-8dc4-8c56e5ac633d"
   },
   "outputs": [],
   "source": [
    "# %cd \"C:\\Users\\rockr\\Desktop\\Code\\SRP\\SRP Code\"\n",
    "# !git clone https://github.com/openai/glide-text2im.git\n",
    "# %pip install -q -e .\n",
    "%cd \"C:\\Users\\rockr\\Desktop\\Code\\SRP\\SRP Code\\glide-text2im\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082ae5f4",
   "metadata": {
    "id": "MDMaMUwRO0Pm"
   },
   "source": [
    "### Set-up functions, models and options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4e3c7c5",
   "metadata": {
    "id": "BCSZT7R2NOGv"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import torch as th\n",
    "\n",
    "from glide_text2im.download import load_checkpoint\n",
    "from glide_text2im.model_creation import (\n",
    "    create_model_and_diffusion,\n",
    "    model_and_diffusion_defaults,\n",
    "    model_and_diffusion_defaults_upsampler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09d1a773",
   "metadata": {
    "id": "a4QBvuzfNOG6"
   },
   "outputs": [],
   "source": [
    "# This notebook supports both CPU and GPU.\n",
    "# On CPU, generating one sample may take on the order of 20 minutes.\n",
    "# On a GPU, it should be under a minute.\n",
    "\n",
    "has_cuda = th.cuda.is_available()\n",
    "device = th.device('cpu' if not has_cuda else 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2d6929",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "cfd0b7ad7ed24c2491e40a27b173c230",
      "6b84156c40384924a8d86d94213fbaff",
      "d2b173c7aad347fb8f5791b924f2d9b8",
      "d696f75f5be442cbba6c5dd6c31b5716",
      "5f989818f91d4e8f96fa2c182b50bb1c",
      "af5af961fe4f4e648784568b89db26e9",
      "b7a4807cd0434bb1b96cf40d485cef69",
      "ccab696cd7994d0fa19f8280370b3d32",
      "c2af0d5d956c421daff2fe86caae732e",
      "7fbc8a0b59944e50bb856424567661fe",
      "48acf713163d40bb9b30ec31e6884103"
     ]
    },
    "id": "7Zt1vrctNOG8",
    "outputId": "88282295-cb04-4c23-87cd-0cf616ec01e8"
   },
   "outputs": [],
   "source": [
    "# Create base model.\n",
    "options = model_and_diffusion_defaults()\n",
    "options['use_fp16'] = has_cuda\n",
    "options['timestep_respacing'] = '50' # use 100 diffusion steps for fast sampling\n",
    "model, diffusion = create_model_and_diffusion(**options)\n",
    "model.eval()\n",
    "if has_cuda:\n",
    "    model.convert_to_fp16()\n",
    "model.to(device)\n",
    "model.load_state_dict(load_checkpoint('base', device))\n",
    "print('total base parameters', sum(x.numel() for x in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d60d9c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "80da2ea2cf814a939b80491f46f1d356",
      "be987294bb86474a8af1be3ca12332e9",
      "74c6566610fa46ad8aade31cfbfde4a8",
      "af07f1c250d2472580af9bde5eebc2d6",
      "e78bbb1eba044a0f9f6b39a0bda40b5c",
      "1436b818057a491bb0dcae1747f5cc89",
      "02f87658f111452a8fe00046fbefd49f",
      "92edd58b3848462cb8a7f81207adcfc2",
      "aed38e49c1df44edbcfde4cbdb314bc5",
      "075d3fb5f96d40b982196554d093f673",
      "534d69c3b6364f48bb2f692f4dc98894"
     ]
    },
    "id": "JoJ_xoYdNOG-",
    "outputId": "38798c34-2b00-4873-c1e0-a63966549fb7"
   },
   "outputs": [],
   "source": [
    "# Create upsampler model.\n",
    "options_up = model_and_diffusion_defaults_upsampler()\n",
    "options_up['use_fp16'] = has_cuda\n",
    "options_up['timestep_respacing'] = 'fast27' # use 27 diffusion steps for very fast sampling\n",
    "model_up, diffusion_up = create_model_and_diffusion(**options_up)\n",
    "model_up.eval()\n",
    "if has_cuda:\n",
    "    model_up.convert_to_fp16()\n",
    "model_up.to(device)\n",
    "model_up.load_state_dict(load_checkpoint('upsample', device))\n",
    "print('total upsampler parameters', sum(x.numel() for x in model_up.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1451ec47",
   "metadata": {
    "id": "U55PSe1zNOHA"
   },
   "outputs": [],
   "source": [
    "def show_images(batch: th.Tensor):\n",
    "    \"\"\" Display a batch of images inline. \"\"\"\n",
    "    scaled = ((batch + 1)*127.5).round().clamp(0,255).to(th.uint8).cpu()\n",
    "    reshaped = scaled.permute(2, 0, 3, 1).reshape([batch.shape[2], -1, 3])\n",
    "    display(Image.fromarray(reshaped.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ae61eb",
   "metadata": {
    "id": "UX0uFu4EQFxS"
   },
   "source": [
    "### Sample at 64x64 resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73e4489",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "9a3f80789d334cd5ae05d11ba96adc18",
      "c789e2ac565c413a9e09a7413a6d6367",
      "a5b5faaeade049fe85280c40a2324e78",
      "d5e791b252b54a40bd852cd803e0cb1b",
      "21f032f6aa894d31a939e690f3c76fee",
      "f65f6d6897ce4e0f80ea559a79ee417b",
      "988029f4449a4574846d699487319310",
      "5eab0b51ea9f41f09a37da03532e4609",
      "67f8cb19a0294a3bb4cc43950a214a49",
      "93503751ab044d88a396e740fe4fc568",
      "80b58fa377fb4af7b04233090a6491ff"
     ]
    },
    "id": "0vL5aXMgNOHF",
    "outputId": "330fb823-9619-458d-a4a7-73a9eaa00b6b"
   },
   "outputs": [],
   "source": [
    "prompt = \"a high quality painting of monkeys\"\n",
    "\n",
    "# Sampling parameters\n",
    "batch_size =  1\n",
    "guidance_scale = 3.0\n",
    "\n",
    "# Tune this parameter to control the sharpness of 256x256 images.\n",
    "# A value of 1.0 is sharper, but sometimes results in grainy artifacts.\n",
    "upsample_temp = 1 #@param {type:\"number\"}\n",
    "\n",
    "##############################\n",
    "# Sample from the base model #\n",
    "##############################\n",
    "\n",
    "# Create the text tokens to feed to the model.\n",
    "tokens = model.tokenizer.encode(prompt)\n",
    "tokens, mask = model.tokenizer.padded_tokens_and_mask(\n",
    "    tokens, options['text_ctx']\n",
    ")\n",
    "\n",
    "# Create the classifier-free guidance tokens (empty)\n",
    "full_batch_size = batch_size * 2\n",
    "uncond_tokens, uncond_mask = model.tokenizer.padded_tokens_and_mask(\n",
    "    [], options['text_ctx']\n",
    ")\n",
    "\n",
    "# Pack the tokens together into model kwargs.\n",
    "model_kwargs = dict(\n",
    "    tokens=th.tensor(\n",
    "        [tokens] * batch_size + [uncond_tokens] * batch_size, device=device\n",
    "    ),\n",
    "    mask=th.tensor(\n",
    "        [mask] * batch_size + [uncond_mask] * batch_size,\n",
    "        dtype=th.bool,\n",
    "        device=device,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Create a classifier-free guidance sampling function\n",
    "def model_fn(x_t, ts, **kwargs):\n",
    "    half = x_t[: len(x_t) // 2]\n",
    "    combined = th.cat([half, half], dim=0)\n",
    "    model_out = model(combined, ts, **kwargs)\n",
    "    eps, rest = model_out[:, :3], model_out[:, 3:]\n",
    "    cond_eps, uncond_eps = th.split(eps, len(eps) // 2, dim=0)\n",
    "    half_eps = uncond_eps + guidance_scale * (cond_eps - uncond_eps)\n",
    "    eps = th.cat([half_eps, half_eps], dim=0)\n",
    "    return th.cat([eps, rest], dim=1)\n",
    "\n",
    "# Sample from the base model.\n",
    "model.del_cache()\n",
    "samples = diffusion.p_sample_loop(\n",
    "    model_fn,\n",
    "    (full_batch_size, 3, options[\"image_size\"], options[\"image_size\"]),\n",
    "    device=device,\n",
    "    clip_denoised=True,\n",
    "    progress=True,\n",
    "    model_kwargs=model_kwargs,\n",
    "    cond_fn=None,\n",
    ")[:batch_size]\n",
    "model.del_cache()\n",
    "\n",
    "show_images(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171c47da",
   "metadata": {
    "id": "h075ElqBPmq8"
   },
   "source": [
    "### Upsample to 256x256 resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47457741",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305,
     "referenced_widgets": [
      "ece09b9a42a0472387be4e9683bc4ecc",
      "29bb4f23507c4c9281cacb8514ca4cb9",
      "f823f874a70f422797fca136ef1ae695",
      "92281fbca09e473a86e6ab5802179014",
      "48c59213f7464f0fb7b1864a23f429e1",
      "36d23e74b5364b0b8746419180754580",
      "02e7e75811444ac5a1d502f748cc10fe",
      "fec28112bbc14548ad643d76a2776244",
      "368c64cef7dc4fcca7f3a5728f1c371c",
      "a6082a6c130f4e43a21d89c624a72fcb",
      "e30d2b9feda74b0692ea8fbcfae30fc7"
     ]
    },
    "id": "FdFBhsSYNOHH",
    "outputId": "29c47104-261c-4afc-b659-d86477343ee2"
   },
   "outputs": [],
   "source": [
    "tokens = model_up.tokenizer.encode(prompt)\n",
    "tokens, mask = model_up.tokenizer.padded_tokens_and_mask(\n",
    "    tokens, options_up['text_ctx']\n",
    ")\n",
    "\n",
    "# Create the model conditioning dict.\n",
    "model_kwargs = dict(\n",
    "    # Low-res image to upsample.\n",
    "    low_res=((samples+1)*127.5).round()/127.5 - 1,\n",
    "\n",
    "    # Text tokens\n",
    "    tokens=th.tensor(\n",
    "        [tokens] * batch_size, device=device\n",
    "    ),\n",
    "    mask=th.tensor(\n",
    "        [mask] * batch_size,\n",
    "        dtype=th.bool,\n",
    "        device=device,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Sample from the base model.\n",
    "model_up.del_cache()\n",
    "up_shape = (batch_size, 3, options_up[\"image_size\"], options_up[\"image_size\"])\n",
    "up_samples = diffusion_up.ddim_sample_loop(\n",
    "    model_up,\n",
    "    up_shape,\n",
    "    noise=th.randn(up_shape, device=device) * upsample_temp,\n",
    "    device=device,\n",
    "    clip_denoised=True,\n",
    "    progress=True,\n",
    "    model_kwargs=model_kwargs,\n",
    "    cond_fn=None,\n",
    ")[:batch_size]\n",
    "model_up.del_cache()\n",
    "\n",
    "\n",
    "show_images(up_samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
