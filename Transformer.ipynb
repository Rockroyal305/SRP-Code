{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60deaaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown # **1** Setup, run this once.\n",
    "from IPython.display import clear_output\n",
    "from google.colab.output import eval_js\n",
    "\n",
    "eval_js('google.colab.output.setIframeHeight(\"250\")')\n",
    "\n",
    "!nvidia-smi\n",
    "!pip install git+https://github.com/afiaka87/CLIP.git\n",
    "!pip install taming-transformers\n",
    "!pip install dalle-pytorch\n",
    "!pip install tokenizers\n",
    "!pip install ftfy\n",
    "!pip install regex\n",
    "!pip install tqdm\n",
    "!git clone https://github.com/afiaka87/dalle-pytorch-pretrained.git\n",
    "!pip install wandb\n",
    "%cd dalle-pytorch-pretrained\n",
    "\n",
    "clear_output()\n",
    "\n",
    "eval_js('google.colab.output.setIframeHeight(\"250\")')\n",
    "\n",
    "#!wget --no-clobber https://www.dropbox.com/s/hl5hyzhyal3vfye/dalle_iconic_butterfly_149.pt\n",
    "%pip install tokenizers\n",
    "from tokenizers import Tokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"/content/dalle-pytorch-pretrained/cc12m_tokenizer.json\")\n",
    "\n",
    "VOCAB_SIZE = tokenizer.get_vocab_size()\n",
    "\n",
    "def tokenize(texts, context_length = 256, add_start = False, add_end = False, truncate_text = False):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    sot_tokens = tokenizer.encode(\"<|startoftext|>\").ids if add_start else []\n",
    "    eot_tokens = tokenizer.encode(\"<|endoftext|>\").ids if add_end else []\n",
    "    all_tokens = [sot_tokens + tokenizer.encode(text).ids + eot_tokens for text in texts]\n",
    "    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n",
    "\n",
    "    for i, tokens in enumerate(all_tokens):\n",
    "        if len(tokens) > context_length:\n",
    "            if truncate_text:\n",
    "                tokens = tokens[:context_length]\n",
    "            else:\n",
    "                raise RuntimeError(f\"Input {texts[i]} is too long for context length {context_length}\")\n",
    "        result[i, :len(tokens)] = torch.tensor(tokens)\n",
    "\n",
    "    return result\n",
    "\n",
    "!wget --no-clobber <dropbox_url>\n",
    "\n",
    "%pip install gpustat\n",
    "!git clone https://github.com/lucidrains/DALLE-pytorch\n",
    "%cd ./DALLE-pytorch/\n",
    "!python3 setup.py install\n",
    "!sudo apt-get -y install llvm-9-dev cmake\n",
    "!git clone https://github.com/microsoft/DeepSpeed.git /tmp/Deepspeed\n",
    "%cd /tmp/Deepspeed\n",
    "!DS_BUILD_SPARSE_ATTN=1 ./install.sh -r\n",
    "\n",
    "!pip install deepspeed\n",
    "\n",
    "%cd /content/\n",
    "!apt-get install pv\n",
    "!apt-get install jq\n",
    "!wget https://raw.githubusercontent.com/tonikelope/megadown/master/megadown -O megadown.sh\n",
    "!chmod 755 megadown.sh\n",
    "\n",
    "clear_output()\n",
    "\n",
    "print(\"Finished, move onto the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3677094d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
